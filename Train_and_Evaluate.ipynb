{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-8ieRULck6n"
      },
      "source": [
        "# Installation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code installs essential libraries for transformer models, dataset handling, model fine-tuning, optimization, and efficient data processing in the Google Colab environment."
      ],
      "metadata": {
        "id": "ECtoPH7T9c3_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7NX5ujSYceZZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets peft\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install trl\n",
        "!pip install optuna\n",
        "!pip install pyarrow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWFL9dM-7A1S"
      },
      "source": [
        "# Huggingface Login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code logs you into your Hugging Face account, enabling seamless access to models, datasets, and other resources directly from the Hugging Face Hub in the Google Colab environment."
      ],
      "metadata": {
        "id": "FR_Cwo9k9jd9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbWuSS8x7CuX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2xbxvsl7ELF"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports a comprehensive set of libraries and modules for deep learning, model fine-tuning, and data processing in the Google Colab environment."
      ],
      "metadata": {
        "id": "aDaVflQk9wqW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfQH_uDh7FVk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, load_dataset\n",
        "from peft import get_peft_model, LoraConfig, TaskType,prepare_model_for_kbit_training\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from google.colab import files\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from trl import SFTTrainer\n",
        "import transformers\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "import optuna\n",
        "from functools import partial\n",
        "import logging\n",
        "import os\n",
        "from huggingface_hub import Repository,create_repo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aViimjYR7F6W"
      },
      "source": [
        "# Upload the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code allow to import the data CSV from your LOCAL to the Google Colab"
      ],
      "metadata": {
        "id": "aorBgxfm9yRQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eD2q9Ms7Im9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "dde31537-c75d-45f7-8d4c-5514f8a3d9f1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b2b03f86-f3ee-439f-a246-8ad75690924c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b2b03f86-f3ee-439f-a246-8ad75690924c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving adapter_model.safetensors to adapter_model.safetensors\n",
            "Saving tokenizer.json to tokenizer.json\n",
            "Saving adapter_config.json to adapter_config.json\n",
            "Saving README.md to README.md\n",
            "Saving special_tokens_map.json to special_tokens_map.json\n",
            "Saving tokenizer_config.json to tokenizer_config.json\n",
            "Saving training_args.bin to training_args.bin\n"
          ]
        }
      ],
      "source": [
        "# Upload an Excel file from your local machine\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQpOhYEE7Msd"
      },
      "source": [
        "# Load Tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code  loads a tokenizer for the specified model, Meta-Llama-3-8B-Instruct, from the Hugging Face Hub. It also customizes the tokenizer by adding special tokens:\n",
        "\n",
        "Pad Token: Set to the end-of-sequence (eos_token) to handle padding during sequence processing.\n",
        "SEP Token: Added as [SEP], which is useful for separating segments in the input."
      ],
      "metadata": {
        "id": "gPb02gZm97IF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Mcr29uVS7N5I"
      },
      "outputs": [],
      "source": [
        "#model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
        "\n",
        "# Load the tokenizer and set pad token\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,padding_side='right')\n",
        "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token,'sep_token':'[SEP]'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_VfMNN3r1x6"
      },
      "source": [
        "# Preprocess the data and create datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code contains functions that help to preprocess the data:\n",
        "\n",
        "*   Remove the models that will be trained later from the data\n",
        "*   Preprocess the row to fit the model's tokenizer\n",
        "*   Create a dataset from the df and fits the columns for the classification task\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pm3oaqX0-DWd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggCg69fh-kRf"
      },
      "outputs": [],
      "source": [
        "def remove_models_from_data(df):\n",
        "  # The models that will be used for inference\n",
        "  models = ['mistralai/Mistral-7B-Instruct-v0.2'\n",
        "          , 'meta-llama/Meta-Llama-3-8B-Instruct']\n",
        "  # Filter out the models from the DataFrame\n",
        "  df = df[~df['model'].isin(models)]\n",
        "  return df\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\n",
        "        f\"{subreddit} [SEP] {conversation} [SEP] {comment} [SEP] {reply}\"\n",
        "        for subreddit, conversation, comment, reply in zip(\n",
        "            examples['subreddit_name'],\n",
        "            examples['conversation_title'],\n",
        "            examples['top_level_text'],\n",
        "            examples['reply_text']\n",
        "        )\n",
        "    ]\n",
        "    return tokenizer(inputs, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "def process_data(df):\n",
        "    # Reset the index of the DataFrame\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Remove models from data (assuming this means some kind of data cleaning)\n",
        "    df = remove_models_from_data(df)\n",
        "\n",
        "    # Modify the 'model' column: set to 1 if not 'human', otherwise set to 0\n",
        "    df['model'] = df['model'].apply(lambda x: 0 if x == 'human' else 1)\n",
        "\n",
        "    # # Remove any ! from data\n",
        "    # df['reply_text'] = df['reply_text'].apply(lambda text:text.replace('!',''))\n",
        "\n",
        "    # Rename the 'model' column to 'labels'\n",
        "    df = df.rename(columns={'model': 'labels'})\n",
        "\n",
        "    # Convert the DataFrame to a Dataset object and apply tokenization\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "    return tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the datasets from the CSV"
      ],
      "metadata": {
        "id": "9FkqQAsJ6R9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates the 3 datasets that are used for hyperparameter-search, train and evaluation."
      ],
      "metadata": {
        "id": "BryzlGx9-80u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = process_data(pd.read_csv(\"train_by_post.csv\"))\n",
        "validation_dataset = process_data(pd.read_csv(\"val_by_post.csv\"))\n",
        "test_dataset = process_data(pd.read_csv(\"test_by_post.csv\"))\n",
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "panVEobWFiz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk0D8iZMrRny"
      },
      "source": [
        "# Create LORA model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function loads a pre-trained sequence classification model with 8-bit precision, applies LoRA fine-tuning for efficient training, and configures the model with the appropriate tokenizer settings. The result is a model optimized for low-precision computation, ready for use in the Google Colab environment."
      ],
      "metadata": {
        "id": "rIPlmygy_OYL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mog6mbgwpmz0"
      },
      "outputs": [],
      "source": [
        "def load_model():\n",
        "  model =  AutoModelForSequenceClassification.from_pretrained(\n",
        "                model_name,\n",
        "                num_labels=2,\n",
        "                quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
        "                trust_remote_code=True\n",
        "          )\n",
        "  lora_config = LoraConfig(\n",
        "  task_type=TaskType.SEQ_CLS,\n",
        "  r=8,\n",
        "  lora_alpha=16,\n",
        "  lora_dropout=0.1,\n",
        "  )\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "  model.config.pad_token_id=tokenizer.pad_token_id\n",
        "  model.config.sep_token_id=tokenizer.sep_token_id\n",
        "\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "  lora_model = get_peft_model(model, lora_config)\n",
        "  return lora_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UmVef6Wr-fH"
      },
      "source": [
        "# Metrices for evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function calculates key evaluation metrics for model predictions, including accuracy, precision, recall, and F1-score, by comparing predicted labels with true labels. These metrics provide a comprehensive assessment of model performance in the Google Colab environment."
      ],
      "metadata": {
        "id": "wyLeQdXC_Vxj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e17t9wWUsEBF"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3MB-Nxx0JeT"
      },
      "source": [
        "# Hyperparameter Search"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs a hyperparameter search by testing three predefined combinations of learning rates and weight decay values over two training epochs. For each trial, it trains a model, evaluates its performance, and saves the trained model along with the results. The validation accuracy and corresponding hyperparameters are logged to a text file and printed to the console, with all models and logs saved in designated directories within the Google Colab environment."
      ],
      "metadata": {
        "id": "MIt1ZNqI_fDI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-gw7ZsucvSt2"
      },
      "outputs": [],
      "source": [
        "# Define the three hyperparameter combinations\n",
        "hyperparameter_combinations = [\n",
        "    {\"learning_rate\": 0.000135, \"weight_decay\": 0.000016},\n",
        "    {\"learning_rate\": 0.000274, \"weight_decay\": 0.000528},\n",
        "    {\"learning_rate\": 0.000080, \"weight_decay\": 0.000080},\n",
        "]\n",
        "\n",
        "# Open the text file for writing\n",
        "with open('hyperparameter_search_results.txt', 'w') as f:\n",
        "\n",
        "    # Loop through the predefined hyperparameter combinations\n",
        "    for i, params in enumerate(hyperparameter_combinations):\n",
        "        learning_rate = params[\"learning_rate\"]\n",
        "        weight_decay = params[\"weight_decay\"]\n",
        "        num_train_epochs = 2  # Fixed to 2 epochs\n",
        "\n",
        "        # Update TrainingArguments with the current hyperparameters\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f'./results/trial_{i+1}',\n",
        "            evaluation_strategy='epoch',\n",
        "            learning_rate=learning_rate,\n",
        "            per_device_train_batch_size=2,\n",
        "            per_device_eval_batch_size=2,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            weight_decay=weight_decay,\n",
        "            logging_dir=f'./logs/trial_{i+1}',\n",
        "            fp16=True,\n",
        "        )\n",
        "\n",
        "        # Initialize a new model instance for each trial\n",
        "        model = load_model()\n",
        "        # Create a new Trainer instance with updated arguments\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset= train_dataset,\n",
        "            eval_dataset= validation_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        # Training\n",
        "        trainer.train()\n",
        "\n",
        "        # Evaluation\n",
        "        metrics = trainer.evaluate()\n",
        "        # Ensure the directory for saving models exists\n",
        "        save_dir = f'./saved_models/trial_{i+1}'\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Save the trained model\n",
        "        trainer.save_model(f'./saved_models/trial_{i+1}')\n",
        "        # Print hyperparameters and their corresponding metric (accuracy)\n",
        "        print(f\"Trial {i+1}:\")\n",
        "        print(f\"  Learning Rate = {learning_rate}, Weight Decay = {weight_decay}\")\n",
        "        print(f\"  Validation Accuracy = {metrics['eval_accuracy']}\\n\")\n",
        "\n",
        "        # Save hyperparameters and their corresponding metric (accuracy) to the text file\n",
        "        f.write(f\"Trial {i+1}:\\n\")\n",
        "        f.write(f\"  Learning Rate = {learning_rate}, Weight Decay = {weight_decay}\\n\")\n",
        "        f.write(f\"  Validation Accuracy = {metrics['eval_accuracy']}\\n\\n\")\n",
        "\n",
        "print(\"Hyperparameter search completed. Results saved to 'hyperparameter_search_results.txt'. Models saved to './saved_models/'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUzKNoHarYy8"
      },
      "source": [
        "# Fine-Tune the Model - Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up and trains a model using a specific set of hyperparameters: a learning rate of 0.000135, weight decay of 0.000016, and two training epochs. The `TrainingArguments` are configured for batch processing, evaluation strategy, logging, and mixed-precision training. After training, the model is saved to a designated directory in the Google Colab environment, ensuring that the directory exists before saving. This process is streamlined for efficient model training and storage."
      ],
      "metadata": {
        "id": "qp7_9w2G_jPE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xgCVg1R1rc3e"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.000135\n",
        "weight_decay = 0.000016\n",
        "num_train_epochs = 2\n",
        "training_args = TrainingArguments(\n",
        "            output_dir=f'./results/our_{model_name}',\n",
        "            evaluation_strategy='epoch',\n",
        "            learning_rate=learning_rate,\n",
        "            per_device_train_batch_size=2,\n",
        "            per_device_eval_batch_size=2,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            weight_decay=weight_decay,\n",
        "            logging_dir=f'./logs/our_{model_name}',\n",
        "            fp16=True,\n",
        "        )\n",
        "model = load_model()\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "# Ensure the directory for saving models exists\n",
        "save_dir = f'./saved_models/our_{model_name}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save the trained model\n",
        "trainer.save_model(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "TTenjiWG_uCG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovvGCEiMr3n6"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "evaluation = trainer.evaluate()\n",
        "print(evaluation)\n",
        "\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = trainer.predict(test_dataset)\n",
        "test_df = test_dataset.to_pandas()\n",
        "test_df['predictions'] = predictions.predictions.argmax(-1)\n",
        "\n",
        "# Select only the relevant columns (excluding input_ids and attention_mask)\n",
        "output_df = test_df[['subreddit_name', 'conversation_title', 'top_level_text', 'reply_text','labels', 'predictions']]\n",
        "\n",
        "# Save the results to a CSV file\n",
        "output_df.to_csv(\"test_predictions.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDsc2MAY20Mx"
      },
      "source": [
        "# Upload model to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code mounts Google Drive to the Google Colab environment and copies the folder containing your trained model from the local environment to a specified location in Google Drive. It dynamically sets the source and destination paths based on the `model_name` variable, ensuring that your model is securely saved for later use. The folder is copied from `./saved_models/{model_name}` to `/content/drive/MyDrive/saved_models/our_{model_name}`."
      ],
      "metadata": {
        "id": "ofsHgqXTATHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ7LIQHk23BM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_folder = f\"./saved_models/{model_name}\"\n",
        "destination_folder = f\"/content/drive/MyDrive/saved_models/our_{model_name}\"\n",
        "\n",
        "# Copy the entire folder\n",
        "shutil.copytree(source_folder, destination_folder)\n",
        "\n",
        "print(f\"Folder {source_folder} copied to {destination_folder}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNGA6ZMF4ZAs"
      },
      "source": [
        "# Load saved model from local"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKTHs6NkBaJx",
        "outputId": "069bd60e-2d34-4299-c870-73417ff55e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, `load_fine_tuned_model`, loads a fine-tuned sequence classification model and its corresponding tokenizer from a specified directory. It then recreates the `Trainer` object with the loaded model and tokenizer, enabling further use or evaluation. This setup facilitates easy restoration of your fine-tuned model in the Google Colab environment. The function is demonstrated with an example that loads a model from the `./Llama` directory."
      ],
      "metadata": {
        "id": "NGBm6TqUAZT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_fine_tuned_model(model_name, save_dir):\n",
        "    # Load the model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Recreate the trainer with the loaded model and tokenizer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    return model, tokenizer, trainer\n",
        "\n",
        "# # Example usage:\n",
        "# save_dir = './Llama'\n",
        "# model, tokenizer, trainer = load_fine_tuned_model(model_name, save_dir)"
      ],
      "metadata": {
        "id": "R3FDLdxN3_MG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "BUBAUaZx6Nen",
        "G3MB-Nxx0JeT"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}